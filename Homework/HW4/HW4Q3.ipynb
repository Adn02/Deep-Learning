{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50cc6e0e-6b93-4a79-be1e-6fae9acbfec5",
   "metadata": {},
   "source": [
    "<h1>Problem 3</h1> \n",
    "<t>Repeat problems 1 and 2, this time trying to translate from French to English. Train the model on the entire dataset and evaluate it on the entire dataset. Report training loss, validation loss, and validation accuracy. Also, try some qualitative validation, asking the network to generate French translations for some English sentences. Which one is more effective, French-to-English or English-to-French?</t>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee2d0f-f501-4d03-b5c8-4215c4887674",
   "metadata": {},
   "source": [
    "<h2>Data Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "25c4a550-d905-4f62-81e9-d556d02573d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import requests\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bba8c0-532c-4cac-ac23-2c7f0b50dbce",
   "metadata": {},
   "source": [
    "<h3>Load Text File:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "80ad9a90-63c4-4216-88cf-e3447c8ca20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "textPath = \"C:/Users/aidan_000/Desktop/UNCC/Github/Intro-to-DL/datasets/text-sequences/E2F.txt\"\n",
    "\n",
    "# Read lines from the text file and extract English-French sentence pairs\n",
    "E2F = []\n",
    "with open(textPath, 'r', encoding='utf-8') as f:\n",
    "    E2F = ast.literal_eval(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb099920-81b5-407b-a084-044246ab7b66",
   "metadata": {},
   "source": [
    "<h3>English and French Dictionary mapping and tokenization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a5ea01c5-3ef3-4318-b513-b9e59d4689b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        # Initialize dictionaries for word to index and index to word mappings\n",
    "        self.word2index = {\"SOS\": SOS_token, \"EOS\": EOS_token}\n",
    "        self.index2word = {SOS_token:\"SOS\", EOS_token: \"EOS\"}\n",
    "        self.word_count = {}  # Keep track of word frequencies\n",
    "        self.n_words = 2  # Start counting from 3 to account for special tokens\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        # Add all words in a sentence to the vocabulary\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # Add a word to the vocabulary\n",
    "        if word not in self.word2index:\n",
    "            # Assign a new index to the word and update mappings\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word_count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            # Increment word count if the word already exists in the vocabulary\n",
    "            self.word_count[word] += 1\n",
    "\n",
    "# Custom Dataset class for English to French sentences\n",
    "class EngFrDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.eng_vocab = Vocabulary()\n",
    "        self.fr_vocab = Vocabulary()\n",
    "        self.pairs = []\n",
    "\n",
    "        # Process each English-French pair\n",
    "        for eng, fr in pairs:\n",
    "            self.eng_vocab.add_sentence(eng)\n",
    "            self.fr_vocab.add_sentence(fr)\n",
    "            self.pairs.append((eng, fr))\n",
    "\n",
    "        # Separate English and French sentences\n",
    "        self.eng_sentences = [pair[0] for pair in self.pairs]\n",
    "        self.fr_sentences = [pair[1] for pair in self.pairs]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of sentence pairs\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the sentences by index\n",
    "        eng_sentence = self.eng_sentences[idx]\n",
    "        fr_sentence = self.fr_sentences[idx]\n",
    "        input_indices = [self.fr_vocab.word2index[word] for word in fr_sentence.split()] + [EOS_token]\n",
    "        target_indices = [self.eng_vocab.word2index[word] for word in eng_sentence.split()] + [EOS_token]\n",
    "        \n",
    "        return torch.tensor(input_indices, dtype=torch.long), torch.tensor(target_indices, dtype=torch.long)\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "dataset = EngFrDataset(E2F)\n",
    "train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "valid_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23f5c00-3945-4905-8d30-892e97db5579",
   "metadata": {},
   "source": [
    "<h2>French to English Encoder/Decoder GRU Model without Attention</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9921ef88-71c7-4b04-b633-d53f47a92bd2",
   "metadata": {},
   "source": [
    "<h3>Model Training and Inferencing Function:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f329f6b-1a23-453a-ad61-10883eff3647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"The Encoder part of the seq2seq model.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Forward pass for the encoder\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        # Initializes hidden state\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"The Decoder part of the seq2seq model.\"\"\"\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "                             \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d191b5d1-1699-4988-87c9-8b2298aa1da6",
   "metadata": {},
   "source": [
    "<h3>Hyperparameters and Training:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ebe0ae5a-80d1-4473-8bfe-7262e08e089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    # Initialize encoder hidden state\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    # Clear gradients for optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Calculate the length of input and target tensors\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # Initialize loss\n",
    "    loss = 0\n",
    "\n",
    "    # Encoding each word in the input\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei].unsqueeze(0), encoder_hidden)\n",
    "\n",
    "    # Decoder's first input is the SOS token\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    # Decoder starts with the encoder's last hidden state\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Decoding loop\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        # Choose top1 word from decoder's output\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # Detach from history as input\n",
    "\n",
    "        # Calculate loss\n",
    "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "        if decoder_input.item() == EOS_token:  # Stop if EOS token is generated\n",
    "            break\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update encoder and decoder parameters\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    # Return average loss\n",
    "    return loss.item() / target_length\n",
    "\n",
    "def training(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, dataloader, epochs, device):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, (input_tensor, target_tensor) in enumerate(dataloader):\n",
    "            # Move tensors to the correct device\n",
    "            input_tensor = input_tensor[0].to(device)\n",
    "            target_tensor = target_tensor[0].to(device)\n",
    "\n",
    "            # Perform a single training step and update total loss\n",
    "            loss = train_fn(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            total_loss += loss\n",
    "\n",
    "        # Print loss every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {total_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fda27085-f2f8-49b8-9238-67e2f8831583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.859666400082552\n",
      "Epoch 10, Loss: 2.49284116267863\n",
      "Epoch 20, Loss: 1.8133120078692553\n",
      "Epoch 30, Loss: 1.3120273689908164\n",
      "Epoch 40, Loss: 0.7633754411264195\n"
     ]
    }
   ],
   "source": [
    "input_size = len(dataset.fr_vocab.word2index)\n",
    "hidden_size = 256\n",
    "output_size =  len(dataset.eng_vocab.word2index)\n",
    "\n",
    "lr = 0.0001\n",
    "epochs = 50\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(hidden_size, output_size).to(device)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "training(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, train_dataloader, epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd752f-48f4-47c4-8284-7f4a0115c0c4",
   "metadata": {},
   "source": [
    "<h3>Evaluating and Comparing Target vs Predictions:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "138ff929-05ba-471c-9bf3-ad48ce2f6026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_show_examples(encoder, decoder, dataloader, criterion, n_examples=10):\n",
    "    # Switch model to evaluation mode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    fr_vocab = dataloader.dataset.fr_vocab\n",
    "    eng_vocab = dataloader.dataset.eng_vocab\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # No gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for i, (input_tensor, target_tensor) in enumerate(dataloader):\n",
    "            # Move tensors to the correct device\n",
    "            input_tensor = input_tensor[0].to(device)\n",
    "            target_tensor = target_tensor[0].to(device)\n",
    "\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "\n",
    "            input_length = input_tensor.size(0)\n",
    "            target_length = target_tensor.size(0)\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            # Encoding step\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(input_tensor[ei].unsqueeze(0), encoder_hidden)\n",
    "\n",
    "            # Decoding step\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            predicted_indices = []\n",
    "\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                predicted_indices.append(topi.item())\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "\n",
    "                loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "                if decoder_input.item() == EOS_token:\n",
    "                    break\n",
    "\n",
    "            # Calculate and print loss and accuracy for the evaluation\n",
    "            total_loss += loss.item() / target_length\n",
    "            if predicted_indices == target_tensor.tolist():\n",
    "                correct_predictions += 1\n",
    "\n",
    "            # Optionally, print some examples\n",
    "            if i < n_examples:\n",
    "                predicted_sentence = ' '.join([eng_vocab.index2word[index] for index in predicted_indices if index not in (SOS_token, EOS_token)])\n",
    "                target_sentence = ' '.join([eng_vocab.index2word[index.item()] for index in target_tensor if index.item() not in (SOS_token, EOS_token)])\n",
    "                input_sentence = ' '.join([fr_vocab.index2word[index.item()] for index in input_tensor if index.item() not in (SOS_token, EOS_token)])\n",
    "\n",
    "                print(f'Input: {input_sentence}, Target: {target_sentence}, Predicted: {predicted_sentence}')\n",
    "\n",
    "        # Print overall evaluation results\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        accuracy = correct_predictions / len(dataloader)\n",
    "        print(f'Evaluation Loss: {average_loss:.4f}, Accuracy: {100*accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9fd2d2fb-6180-4fdc-b760-5b648a4b1862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Il étudie l'histoire, Target: He studies history, Predicted: He studies history\n",
      "Input: Elle écrit de la poésie pendant son temps libre, Target: She writes poetry in her free time, Predicted: She writes poetry in her free free time\n",
      "Input: Nous jouons de la musique au concert, Target: We play music at the concert, Predicted: We play music at the concert\n",
      "Input: Nous voyageons en train, Target: We travel by train, Predicted: We travel by train\n",
      "Input: Ils parlent différentes langues, Target: They speak different languages, Predicted: They speak different languages\n",
      "Input: Ils voyagent autour du monde, Target: They travel around the world, Predicted: They travel around the world\n",
      "Input: Nous parlons au téléphone, Target: We talk on the phone, Predicted: We talk on the phone\n",
      "Input: Elle rêve de voler, Target: She dreams of flying, Predicted: She dreams of flying\n",
      "Input: Ils nagent dans la piscine, Target: They swim in the pool, Predicted: They swim in the pool\n",
      "Input: Il chante dans le chœur, Target: He sings in the choir, Predicted: He sings in the choir\n",
      "Evaluation Loss: 0.3804, Accuracy: 96.10%\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_show_examples(encoder, decoder, valid_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95741572-6d7f-49cf-a516-4bcb82284b66",
   "metadata": {},
   "source": [
    "<h2>French to English Encoder/Decoder GRU Model with Attention</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28577ae-ca29-4f67-8a91-2a4c08bb2762",
   "metadata": {},
   "source": [
    "<h3>Model Training and Inferencing Function:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e7c02b0-f08c-4097-b436-14ad0b0b3cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"The Encoder part of the seq2seq model.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Forward pass for the encoder\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        # Initializes hidden state\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "class AttnDecoder(nn.Module):\n",
    "    \"\"\"Decoder with attention mechanism.\"\"\"\n",
    "    def __init__(self, hidden_size, output_size, max_length, dropout_p=0.1):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        # Attention weights\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        # Combine embedded input and context vector\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attn_weights = torch.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        # Apply attention weights to encoder outputs to get context\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = torch.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = torch.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f84be-eb02-4743-b364-ec6115ba2c8a",
   "metadata": {},
   "source": [
    "<h3>Hyperparameters and Training:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cfd88d3f-003d-4d1e-b722-02744247e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_train_fn(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # Encode each character in the input\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei].unsqueeze(0), encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    # Decoder's first input is the SOS token\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    # Initial decoder hidden state is encoder's last hidden state\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Decoder with attention\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # Detach from history as input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "def training(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, dataloader, epochs, device):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, (input_tensor, target_tensor) in enumerate(dataloader):\n",
    "            # Move tensors to the correct device\n",
    "            input_tensor = input_tensor[0].to(device)\n",
    "            target_tensor = target_tensor[0].to(device)\n",
    "\n",
    "            # Perform a single training step and update total loss\n",
    "            loss = attn_train_fn(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\n",
    "            total_loss += loss\n",
    "\n",
    "        # Print loss every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {total_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "79b7d6e8-d5f2-4b67-98eb-12e8f9b42d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.733839702812624\n",
      "Epoch 10, Loss: 2.3674116421129203\n",
      "Epoch 20, Loss: 1.42018545785655\n",
      "Epoch 30, Loss: 0.7307021817935417\n",
      "Epoch 40, Loss: 0.34352856872622883\n"
     ]
    }
   ],
   "source": [
    "input_size = len(dataset.fr_vocab.word2index)\n",
    "hidden_size = 256\n",
    "output_size =  len(dataset.eng_vocab.word2index)\n",
    "max_length = 14\n",
    "\n",
    "lr = 0.0001\n",
    "epochs = 50\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "attn_decoder = AttnDecoder(hidden_size, output_size, max_length).to(device)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = optim.Adam(attn_decoder.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "training(encoder, attn_decoder, encoder_optimizer, decoder_optimizer, criterion, train_dataloader, epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f98f749-528b-48de-ae8d-7d469ecb6fb7",
   "metadata": {},
   "source": [
    "<h3>Evaluating and Comparing Target vs Predictions:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "844101a4-ab84-4a87-8574-2a4c5dbd7fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_show_examples(encoder, decoder, dataloader, criterion, n_examples=10):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    fr_vocab = dataloader.dataset.fr_vocab\n",
    "    eng_vocab = dataloader.dataset.eng_vocab\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (input_tensor, target_tensor) in enumerate(dataloader):\n",
    "            input_tensor = input_tensor[0].to(device)\n",
    "            target_tensor = target_tensor[0].to(device)\n",
    "\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            input_length = input_tensor.size(0)\n",
    "            target_length = target_tensor.size(0)\n",
    "\n",
    "            encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            # Encode input\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(input_tensor[ei].unsqueeze(0), encoder_hidden)\n",
    "                encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "            # Decode with attention\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            predicted_indices = []\n",
    "\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                predicted_indices.append(topi.item())\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "\n",
    "                loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "                if decoder_input.item() == EOS_token:\n",
    "                    break\n",
    "\n",
    "            total_loss += loss.item() / target_length\n",
    "            if predicted_indices == target_tensor.tolist():\n",
    "                correct_predictions += 1\n",
    "\n",
    "            # Optionally, print some examples\n",
    "            if i < n_examples:\n",
    "                predicted_sentence = ' '.join([eng_vocab.index2word[index] for index in predicted_indices if index not in (SOS_token, EOS_token)])\n",
    "                target_sentence = ' '.join([eng_vocab.index2word[index.item()] for index in target_tensor if index.item() not in (SOS_token, EOS_token)])\n",
    "                input_sentence = ' '.join([fr_vocab.index2word[index.item()] for index in input_tensor if index.item() not in (SOS_token, EOS_token)])\n",
    "\n",
    "                print(f'Input: {input_sentence}, Target: {target_sentence}, Predicted: {predicted_sentence}')\n",
    "\n",
    "        # Print overall evaluation results\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        accuracy = correct_predictions / len(dataloader)\n",
    "        print(f'Evaluation Loss: {average_loss:.4f}, Accuracy: {100*accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "17106bdf-50c0-4a80-ad14-0145ae1ad6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Le bébé pleure, Target: The baby cries, Predicted: The baby cries\n",
      "Input: Le chat dort, Target: The cat is sleeping, Predicted: The cat is sleeping\n",
      "Input: Il a faim, Target: He is hungry, Predicted: He is hungry\n",
      "Input: Elle enseigne l'anglais à l'école, Target: She teaches English at school, Predicted: She teaches English at school\n",
      "Input: L'horloge tic-tac bruyamment, Target: The clock ticks loudly, Predicted: The clock ticks loudly\n",
      "Input: Tu es fatigué, Target: You are tired, Predicted: You are tired\n",
      "Input: Elle peint un tableau, Target: She paints a picture, Predicted: She paints a picture\n",
      "Input: Il écrit une lettre, Target: He writes a letter, Predicted: He writes a letter\n",
      "Input: Elle écrit de la poésie pendant son temps libre, Target: She writes poetry in her free time, Predicted: She writes poetry in her free time\n",
      "Input: Ils lisent des livres à la bibliothèque, Target: They read books at the library, Predicted: They read books at the library\n",
      "Evaluation Loss: 0.1632, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_show_examples(encoder, attn_decoder, valid_dataloader, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
