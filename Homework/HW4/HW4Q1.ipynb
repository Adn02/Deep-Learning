{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50cc6e0e-6b93-4a79-be1e-6fae9acbfec5",
   "metadata": {},
   "source": [
    "<h1>Problem 1</h1> \n",
    "<t>Developed a GRU-based encoder-decoder architecture for English to French Translation. Train the model on the entire dataset and evaluate it. Report training loss, validation loss, and validation accuracy. Also, try some qualitative validation, asking the network to generate French translations for some English sentences.</t>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee2d0f-f501-4d03-b5c8-4215c4887674",
   "metadata": {},
   "source": [
    "<h2>Data Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25c4a550-d905-4f62-81e9-d556d02573d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import requests\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bba8c0-532c-4cac-ac23-2c7f0b50dbce",
   "metadata": {},
   "source": [
    "<h3>Load Text File:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80ad9a90-63c4-4216-88cf-e3447c8ca20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "textPath = \"C:/Users/aidan_000/Desktop/UNCC/Github/Intro-to-DL/datasets/text-sequences/E2F.txt\"\n",
    "\n",
    "# Read lines from the text file and extract English-French sentence pairs\n",
    "E2F = []\n",
    "with open(textPath, 'r', encoding='utf-8') as f:\n",
    "    E2F = ast.literal_eval(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb099920-81b5-407b-a084-044246ab7b66",
   "metadata": {},
   "source": [
    "<h3>English and French Dictionary mapping and tokenization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5ea01c5-3ef3-4318-b513-b9e59d4689b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        # Initialize dictionaries for word to index and index to word mappings\n",
    "        self.word2index = {\"SOS\": SOS_token, \"EOS\": EOS_token}\n",
    "        self.index2word = {SOS_token:\"SOS\", EOS_token: \"EOS\"}\n",
    "        self.word_count = {}  # Keep track of word frequencies\n",
    "        self.n_words = 2  # Start counting from 3 to account for special tokens\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        # Add all words in a sentence to the vocabulary\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # Add a word to the vocabulary\n",
    "        if word not in self.word2index:\n",
    "            # Assign a new index to the word and update mappings\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word_count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            # Increment word count if the word already exists in the vocabulary\n",
    "            self.word_count[word] += 1\n",
    "\n",
    "# Custom Dataset class for English to French sentences\n",
    "class EngFrDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.eng_vocab = Vocabulary()\n",
    "        self.fr_vocab = Vocabulary()\n",
    "        self.pairs = []\n",
    "\n",
    "        # Process each English-French pair\n",
    "        for eng, fr in pairs:\n",
    "            self.eng_vocab.add_sentence(eng)\n",
    "            self.fr_vocab.add_sentence(fr)\n",
    "            self.pairs.append((eng, fr))\n",
    "\n",
    "        # Separate English and French sentences\n",
    "        self.eng_sentences = [pair[0] for pair in self.pairs]\n",
    "        self.fr_sentences = [pair[1] for pair in self.pairs]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of sentence pairs\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the sentences by index\n",
    "        eng_sentence = self.eng_sentences[idx]\n",
    "        fr_sentence = self.fr_sentences[idx]\n",
    "        input_indices = [self.eng_vocab.word2index[word] for word in eng_sentence.split()] + [EOS_token]\n",
    "        target_indices = [self.fr_vocab.word2index[word] for word in fr_sentence.split()] + [EOS_token]\n",
    "        \n",
    "        return torch.tensor(input_indices, dtype=torch.long), torch.tensor(target_indices, dtype=torch.long)\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "dataset = EngFrDataset(E2F)\n",
    "train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "valid_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23f5c00-3945-4905-8d30-892e97db5579",
   "metadata": {},
   "source": [
    "<h2>English to French Encoder/Decoder GRU Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9921ef88-71c7-4b04-b633-d53f47a92bd2",
   "metadata": {},
   "source": [
    "<h3>Model Training and Inferencing Function:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f329f6b-1a23-453a-ad61-10883eff3647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"The Encoder part of the seq2seq model.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Forward pass for the encoder\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        # Initializes hidden state\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"The Decoder part of the seq2seq model.\"\"\"\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "                             \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d191b5d1-1699-4988-87c9-8b2298aa1da6",
   "metadata": {},
   "source": [
    "<h3>Hyperparameters and Training:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebe0ae5a-80d1-4473-8bfe-7262e08e089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    # Initialize encoder hidden state\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    # Clear gradients for optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Calculate the length of input and target tensors\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # Initialize loss\n",
    "    loss = 0\n",
    "\n",
    "    # Encoding each word in the input\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei].unsqueeze(0), encoder_hidden)\n",
    "\n",
    "    # Decoder's first input is the SOS token\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    # Decoder starts with the encoder's last hidden state\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Decoding loop\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        # Choose top1 word from decoder's output\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # Detach from history as input\n",
    "\n",
    "        # Calculate loss\n",
    "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "        if decoder_input.item() == EOS_token:  # Stop if EOS token is generated\n",
    "            break\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update encoder and decoder parameters\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    # Return average loss\n",
    "    return loss.item() / target_length\n",
    "\n",
    "def training(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, dataloader, epochs, device):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, (input_tensor, target_tensor) in enumerate(dataloader):\n",
    "            # Move tensors to the correct device\n",
    "            input_tensor = input_tensor[0].to(device)\n",
    "            target_tensor = target_tensor[0].to(device)\n",
    "\n",
    "            # Perform a single training step and update total loss\n",
    "            loss = train_fn(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            total_loss += loss\n",
    "\n",
    "        # Print loss every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {total_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fda27085-f2f8-49b8-9238-67e2f8831583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.484506012704835\n",
      "Epoch 10, Loss: 2.3775603550435385\n",
      "Epoch 20, Loss: 1.7563362155912146\n",
      "Epoch 30, Loss: 1.2835273592896224\n",
      "Epoch 40, Loss: 0.8404270105523145\n"
     ]
    }
   ],
   "source": [
    "input_size = len(dataset.eng_vocab.word2index)\n",
    "hidden_size = 256\n",
    "output_size =  len(dataset.fr_vocab.word2index)\n",
    "\n",
    "lr = 0.0001\n",
    "epochs = 50\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(hidden_size, output_size).to(device)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "training(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, train_dataloader, epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd752f-48f4-47c4-8284-7f4a0115c0c4",
   "metadata": {},
   "source": [
    "<h3>Evaluating and Comparing Target vs Predictions:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "138ff929-05ba-471c-9bf3-ad48ce2f6026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_show_examples(encoder, decoder, dataloader, criterion, n_examples=10):\n",
    "    # Switch model to evaluation mode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    fr_vocab = dataloader.dataset.fr_vocab\n",
    "    eng_vocab = dataloader.dataset.eng_vocab\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # No gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for i, (input_tensor, target_tensor) in enumerate(dataloader):\n",
    "            # Move tensors to the correct device\n",
    "            input_tensor = input_tensor[0].to(device)\n",
    "            target_tensor = target_tensor[0].to(device)\n",
    "\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "\n",
    "            input_length = input_tensor.size(0)\n",
    "            target_length = target_tensor.size(0)\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            # Encoding step\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(input_tensor[ei].unsqueeze(0), encoder_hidden)\n",
    "\n",
    "            # Decoding step\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            predicted_indices = []\n",
    "\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                predicted_indices.append(topi.item())\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "\n",
    "                loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "                if decoder_input.item() == EOS_token:\n",
    "                    break\n",
    "\n",
    "            # Calculate and print loss and accuracy for the evaluation\n",
    "            total_loss += loss.item() / target_length\n",
    "            if predicted_indices == target_tensor.tolist():\n",
    "                correct_predictions += 1\n",
    "\n",
    "            # Optionally, print some examples\n",
    "            if i < n_examples:\n",
    "                predicted_sentence = ' '.join([fr_vocab.index2word[index] for index in predicted_indices if index not in (SOS_token, EOS_token)])\n",
    "                target_sentence = ' '.join([fr_vocab.index2word[index.item()] for index in target_tensor if index.item() not in (SOS_token, EOS_token)])\n",
    "                input_sentence = ' '.join([eng_vocab.index2word[index.item()] for index in input_tensor if index.item() not in (SOS_token, EOS_token)])\n",
    "\n",
    "                print(f'Input: {input_sentence}, Target: {target_sentence}, Predicted: {predicted_sentence}')\n",
    "\n",
    "        # Print overall evaluation results\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        accuracy = correct_predictions / len(dataloader)\n",
    "        print(f'Evaluation Loss: {average_loss:.4f}, Accuracy: {100*accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fd2d2fb-6180-4fdc-b760-5b648a4b1862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The flowers bloom in spring, Target: Les fleurs fleurissent au printemps, Predicted: Les fleurs fleurissent au printemps\n",
      "Input: The wind blows gently, Target: Le vent souffle doucement, Predicted: Le vent souffle doucement\n",
      "Input: He studies history, Target: Il étudie l'histoire, Predicted: Il étudie l'histoire\n",
      "Input: She teaches English at school, Target: Elle enseigne l'anglais à l'école, Predicted: Elle enseigne l'anglais à l'école\n",
      "Input: They travel to Paris, Target: Ils voyagent à Paris, Predicted: Ils voyagent à Paris\n",
      "Input: She is happy, Target: Elle est heureuse, Predicted: Elle est heureuse\n",
      "Input: We build a sandcastle, Target: Nous construisons un château de sable, Predicted: Nous construisons un château de voler\n",
      "Input: The children play in the park, Target: Les enfants jouent dans le parc, Predicted: Les enfants jouent dans le parc\n",
      "Input: The restaurant serves delicious food, Target: Le restaurant sert une délicieuse cuisine, Predicted: Le restaurant sert une délicieuse cuisine\n",
      "Input: They hike in the forest, Target: Ils font de la randonnée dans la forêt, Predicted: Ils font de la randonnée\n",
      "Evaluation Loss: 0.4735, Accuracy: 77.92%\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_show_examples(encoder, decoder, valid_dataloader, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
