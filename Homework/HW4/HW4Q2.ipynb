{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50cc6e0e-6b93-4a79-be1e-6fae9acbfec5",
   "metadata": {},
   "source": [
    "<h1>Problem 2</h1> \n",
    "<t>Repeat problem 1; this time, extend the network with attention. Train the model on the entire dataset and evaluate it. Report training loss, validation loss, and validation accuracy. Also, try some qualitative validation, asking the network to generate French translations for some English sentences. Also, the results were compared against problem 1.</t>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee2d0f-f501-4d03-b5c8-4215c4887674",
   "metadata": {},
   "source": [
    "<h2>Data Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25c4a550-d905-4f62-81e9-d556d02573d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import requests\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bba8c0-532c-4cac-ac23-2c7f0b50dbce",
   "metadata": {},
   "source": [
    "<h3>Load Text File:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "80ad9a90-63c4-4216-88cf-e3447c8ca20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "textPath = \"C:/Users/aidan_000/Desktop/UNCC/Github/Intro-to-DL/datasets/text-sequences/E2F.txt\"\n",
    "\n",
    "# Read lines from the text file and extract English-French sentence pairs\n",
    "E2F = []\n",
    "with open(textPath, 'r', encoding='utf-8') as f:\n",
    "    E2F = ast.literal_eval(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb099920-81b5-407b-a084-044246ab7b66",
   "metadata": {},
   "source": [
    "<h3>English and French Dictionary mapping and tokenization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a5ea01c5-3ef3-4318-b513-b9e59d4689b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        # Initialize dictionaries for word to index and index to word mappings\n",
    "        self.word2index = {\"SOS\": SOS_token, \"EOS\": EOS_token}\n",
    "        self.index2word = {SOS_token:\"SOS\", EOS_token: \"EOS\"}\n",
    "        self.word_count = {}  # Keep track of word frequencies\n",
    "        self.n_words = 2  # Start counting from 3 to account for special tokens\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        # Add all words in a sentence to the vocabulary\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # Add a word to the vocabulary\n",
    "        if word not in self.word2index:\n",
    "            # Assign a new index to the word and update mappings\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word_count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            # Increment word count if the word already exists in the vocabulary\n",
    "            self.word_count[word] += 1\n",
    "\n",
    "# Custom Dataset class for English to French sentences\n",
    "class EngFrDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.eng_vocab = Vocabulary()\n",
    "        self.fr_vocab = Vocabulary()\n",
    "        self.pairs = []\n",
    "\n",
    "        # Process each English-French pair\n",
    "        for eng, fr in pairs:\n",
    "            self.eng_vocab.add_sentence(eng)\n",
    "            self.fr_vocab.add_sentence(fr)\n",
    "            self.pairs.append((eng, fr))\n",
    "\n",
    "        # Separate English and French sentences\n",
    "        self.eng_sentences = [pair[0] for pair in self.pairs]\n",
    "        self.fr_sentences = [pair[1] for pair in self.pairs]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of sentence pairs\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the sentences by index\n",
    "        eng_sentence = self.eng_sentences[idx]\n",
    "        fr_sentence = self.fr_sentences[idx]\n",
    "        input_indices = [self.eng_vocab.word2index[word] for word in eng_sentence.split()] + [EOS_token]\n",
    "        target_indices = [self.fr_vocab.word2index[word] for word in fr_sentence.split()] + [EOS_token]\n",
    "        \n",
    "        return torch.tensor(input_indices, dtype=torch.long), torch.tensor(target_indices, dtype=torch.long)\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "dataset = EngFrDataset(E2F)\n",
    "train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "valid_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23f5c00-3945-4905-8d30-892e97db5579",
   "metadata": {},
   "source": [
    "<h2>English to French Encoder/Decoder GRU Model with Attention</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9921ef88-71c7-4b04-b633-d53f47a92bd2",
   "metadata": {},
   "source": [
    "<h3>Model Training and Inferencing Function:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f329f6b-1a23-453a-ad61-10883eff3647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"The Encoder part of the seq2seq model.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Forward pass for the encoder\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        # Initializes hidden state\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "class AttnDecoder(nn.Module):\n",
    "    \"\"\"Decoder with attention mechanism.\"\"\"\n",
    "    def __init__(self, hidden_size, output_size, max_length, dropout_p=0.1):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        # Attention weights\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        # Combine embedded input and context vector\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attn_weights = torch.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        # Apply attention weights to encoder outputs to get context\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = torch.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = torch.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d191b5d1-1699-4988-87c9-8b2298aa1da6",
   "metadata": {},
   "source": [
    "<h3>Hyperparameters and Training:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ebe0ae5a-80d1-4473-8bfe-7262e08e089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # Encode each character in the input\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei].unsqueeze(0), encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    # Decoder's first input is the SOS token\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    # Initial decoder hidden state is encoder's last hidden state\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Decoder with attention\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # Detach from history as input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "def training(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, dataloader, epochs, device):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, (input_tensor, target_tensor) in enumerate(dataloader):\n",
    "            # Move tensors to the correct device\n",
    "            input_tensor = input_tensor[0].to(device)\n",
    "            target_tensor = target_tensor[0].to(device)\n",
    "\n",
    "            # Perform a single training step and update total loss\n",
    "            loss = train_fn(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\n",
    "            total_loss += loss\n",
    "\n",
    "        # Print loss every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {total_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fda27085-f2f8-49b8-9238-67e2f8831583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.1341000359860973\n",
      "Epoch 10, Loss: 2.1611277950299055\n",
      "Epoch 20, Loss: 1.491284217993713\n",
      "Epoch 30, Loss: 0.8648059211067748\n",
      "Epoch 40, Loss: 0.4532417903110381\n"
     ]
    }
   ],
   "source": [
    "input_size = len(dataset.eng_vocab.word2index)\n",
    "hidden_size = 256\n",
    "output_size =  len(dataset.fr_vocab.word2index)\n",
    "max_length = 14\n",
    "\n",
    "lr = 0.0001\n",
    "epochs = 50\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = AttnDecoder(hidden_size, output_size, max_length).to(device)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "training(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, train_dataloader, epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd752f-48f4-47c4-8284-7f4a0115c0c4",
   "metadata": {},
   "source": [
    "<h3>Evaluating and Comparing Target vs Predictions:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "138ff929-05ba-471c-9bf3-ad48ce2f6026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_show_examples(encoder, decoder, dataloader, criterion, n_examples=5):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    fr_vocab = dataloader.dataset.fr_vocab\n",
    "    eng_vocab = dataloader.dataset.eng_vocab\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (input_tensor, target_tensor) in enumerate(dataloader):\n",
    "            input_tensor = input_tensor[0].to(device)\n",
    "            target_tensor = target_tensor[0].to(device)\n",
    "\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            input_length = input_tensor.size(0)\n",
    "            target_length = target_tensor.size(0)\n",
    "\n",
    "            encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            # Encode input\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(input_tensor[ei].unsqueeze(0), encoder_hidden)\n",
    "                encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "            # Decode with attention\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            predicted_indices = []\n",
    "\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                predicted_indices.append(topi.item())\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "\n",
    "                loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "                if decoder_input.item() == EOS_token:\n",
    "                    break\n",
    "\n",
    "            total_loss += loss.item() / target_length\n",
    "            if predicted_indices == target_tensor.tolist():\n",
    "                correct_predictions += 1\n",
    "\n",
    "            # Optionally, print some examples\n",
    "            if i < n_examples:\n",
    "                predicted_sentence = ' '.join([fr_vocab.index2word[index] for index in predicted_indices if index not in (SOS_token, EOS_token)])\n",
    "                target_sentence = ' '.join([fr_vocab.index2word[index.item()] for index in target_tensor if index.item() not in (SOS_token, EOS_token)])\n",
    "                input_sentence = ' '.join([eng_vocab.index2word[index.item()] for index in input_tensor if index.item() not in (SOS_token, EOS_token)])\n",
    "\n",
    "                print(f'Input: {input_sentence}, Target: {target_sentence}, Predicted: {predicted_sentence}')\n",
    "\n",
    "        # Print overall evaluation results\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        accuracy = correct_predictions / len(dataloader)\n",
    "        print(f'Evaluation Loss: {average_loss:.4f}, Accuracy: {100*accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9fd2d2fb-6180-4fdc-b760-5b648a4b1862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: We eat breakfast together, Target: Nous prenons le petit déjeuner ensemble, Predicted: Nous prenons le petit déjeuner\n",
      "Input: The sun sets in the evening, Target: Le soleil se couche le soir, Predicted: Le soleil se couche le soir\n",
      "Input: We watch movies on Fridays, Target: Nous regardons des films le vendredi, Predicted: Nous regardons des films le vendredi\n",
      "Input: They travel to Paris, Target: Ils voyagent à Paris, Predicted: Ils voyagent à Paris\n",
      "Input: She wears a red dress, Target: Elle porte une robe rouge, Predicted: Elle porte une robe rouge\n",
      "Evaluation Loss: 0.2265, Accuracy: 94.81%\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_show_examples(encoder, decoder, valid_dataloader, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
